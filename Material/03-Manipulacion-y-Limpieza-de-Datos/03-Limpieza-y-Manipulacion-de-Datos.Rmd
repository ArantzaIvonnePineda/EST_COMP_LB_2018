---
title: "Limpieza y Manipulación de Datos"
author: "León Berdichevsky Acosta"
date: "21 de agosto de 2018"
output:
  html_document: default
  html_notebook:
    css: ../codigo-estilos/cajas.css
    theme: spacelab
---

```{r setup, include = FALSE}
library(tidyverse)
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  fig.align = "center"
)
comma <- function(x) format(x, digits = 2, big.mark = ",")
theme_set(theme_minimal())
```

En estas notas abordaremos los temas de limpieza y manipulación de datos utilizando el paquete `tidyverse` de R. Trataremos los siguientes puntos:

* Los principios de datos limpios.
* Aspectos básicos de manipulación de datos.
* Reestructuración de bases de datos.
* Estrategia divide-aplica-combina.

Es sabido que limpieza y preparación de datos ocupan gran parte del tiempo del 
análisis de datos ([Dasu y Johnson, 2003](http://onlinelibrary.wiley.com/book/10.1002/0471448354) 
y [NYT's ‘Janitor Work’ Is Key Hurdle to Insights](https://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html?mcubz=0)); es por ello que vale la pena dedicar un tiempo a
aprender técnicas que faciliten estas tareas, y entender con qué estructura de los 
datos es más conveniente trabajar.



### Datos Limpios
Una vez que importamos datos a R es conveniente limpiarlos, esto implica 
almacenarlos de una manera consisistente que nos permita
enfocarnos en responder preguntas con los datos en lugar de estar luchando 
con los datos. Entonces, **datos limpios** son datos que facilitan las tareas del 
análisis de datos: 

* **Manipulación:** Manipulación de variables como agregar, filtrar, reordenar,
transformar. 

* **Visualización:** Resúmenes de datos usando gráficas, análisis exploratorio, 
o presentación de resultados. 

* **Modelación:** Ajustar modelos es sencillo si los datos están en la forma 
correcta.


Los principios de **datos limpios** ([Tidy Data de Hadley Wickham](http://vita.had.co.nz/papers/tidy-data.pdf)) proveen una manera 
estándar de organizar la información.

Para enunciarlos, es necesario introducir el concepto de Unidad Observacional. Una **Unidad Observacional** (o Unidad de Observación) es la entidad mayor, primaria o representativa de lo que va a ser objeto específico de estudio en una medición y se refiere al qué o quién es objeto de interés en una investigación. A este objeto de estudio le medimos ciertas características. Una unidad observacional tiene las siguientes propiedades: 

* Una unidad observacional en una tabla puede estar definida por una o más variables. 
* El resto de las variables de la tabla corresponden a las características de la unidad observacional.

Los principios de datos limpios son:

1. Cada variable forma una columna.
2. Cada tipo de unidad observacional forma una tabla.
3. Cada valor de la unidad observacional forma un renglón.
 

Vale la pena notar que los principios de datos limpios se pueden ver como 
teoría de algebra relacional para estadísticos; estos principios equivalen a 
la tercera forma normal de Codd con enfoque en una sola tabla de datos en 
lugar de muchas conectadas en bases de datos relacionales. 

La mayor parte de las bases de datos en estadística tienen forma de tabla rectangular, es decir, están constituidas por datos representados de manera tabular. En el presente curso consideraremos únicamente datos con esta estructura.

**Ejemplo:** Considere la siguiente tabla que representa a 3 individuos expuestos a uno o dos tipos de tratamientos (denotados A y B) y en la que se muestra el resultado del tratamiento correspondiente.

||tratamientoA|tratamientoB
----|------------|---------
Juan Aguirre|- |2
Ana Bernal  |16|11
José López  |3 |1

¿Cuántas variables tiene la tabla? La respuesta más sencilla es 3 variables: nombre, tipo de tratamiento y resultado del tratamiento.

La tabla anterior también se puede reestructurar de la siguiente manera:

 ||Juan Aguirre| Ana Bernal|José López
--|------------|-----------|----------
tratamientoA|- |    16     |   3
tratamientoB|2 |    11     |   1

¿Cumplen las tablas anteriores los principios de datos limpios? La respuesta depende del número de variables y de la definición de unidad observacional.

En este caso, independientemente de la definición de unidad observacional, es claro que la respuesta es NO, ya que ambas tablas violan el principio número 1.

Reestructurando la tabla, obtenemos una nueva tabla que cumple con el principio número 1 de datos limpios: 

nombre|tipo de tratamiento|resultado del tratamiento
------------|-----|---------
Juan Aguirre|a    |-
Ana Bernal  |a    |16
José López  |a    |3
Juan Aguirre|b    |2
Ana Bernal  |b    |11
José López  |b    |1

¿Esta tabla cumple con los principios de datos limpios? Por construcción cumple con el principio número 1. Sin embargo, los principios 2 y 3 dependen de la definición de unidad observacional. En este caso, si la unidad observacional está definida por las variables nombre y tipo de tratamiento, es claro que la tabla cumple con los principios de datos limpios. Sin embargo, una definición diferente de unidad observacional podría implicar que la tabla no cumple con dichos principios.

**Ejercicio:** Define una unidad observacional para el ejemplo anterior tal que la última tabla no cumpla con el principio de datos limpios. Reestructura la tabla en una nueva tabla que sí cumpla con los principios de datos limpios.



### Aspectos básicos de manipulación de datos
Para abordar el tema de Limpieza de Datos, es necesario familiarizarse con aspectos básicos de manipulación de datos. Ilustraremos estos aspectos utilizando funciones que se encuentran en el paquete `tidyverse`. Más adelante regresaremos de lleno al tema de Manipulación de Datos cuando veamos la Estategia divide-aplica-combina.
```{r}
library(tidyverse)
```

Un _tibble_ es un Data Frame en R adaptado al  paquete `tidyverse` mediante el paquete `tibble`. Creamos un tibble utilizando la función con el mismo nombre:
```{r}
df_ej <- tibble(genero = c("mujer", "hombre", "mujer", "mujer", "hombre"), 
                estatura = c(1.65, 1.80, 1.70, 1.60, 1.67))
df_ej
str(df_ej)
```

En específico, estudiaremos las funciones del paquete `dplyr` que es parte del paquete `tidyverse` para manipular _datos limpios_: filtrar, seleccionar, arreglar, mutar y realizar sumarizados. El objeto de entrada de todas estas funciones es un `tibble` y el objeto de salida es también un `tibble`.

#### Filtrar
Obtiene un subconjunto de las filas de acuerdo a un criterio:
```{r}
filter(df_ej, genero == "mujer")
filter(df_ej, estatura > 1.65 & estatura < 1.75)
```

#### Seleccionar
Selecciona columnas de acuerdo al nombre:
```{r}
df_ej
select(df_ej, genero)
select(df_ej, -genero) #Elige todas las columnas excepto la llamada "genero"
select(df_ej, starts_with("g")) #Nombre empieza con la letra "g"
select(df_ej, contains("g")) #Nombre contiene la letra "g"
```

#### Arreglar
Reordena las filas:
```{r}
arrange(df_ej, genero) #Orden alfabético
arrange(df_ej, desc(estatura)) #Orden numérico descendente
```

#### Mutar
Crea y agrega nuevas variables:
```{r}
mutate(df_ej, estatura_cm = estatura * 100) 
mutate(df_ej, estatura_cm = estatura * 100, estatura_in = estatura_cm * 0.3937) 
```

#### Sumarizados
Crea nuevas bases de datos con resúmenes o agregaciones de los datos originales:
```{r}
summarise(df_ej, promedio = mean(estatura))
```
Podemos hacer resúmenes por grupo. Para ello primero creamos una base de datos con información de _agrupación_ con respecto a la variable género (nota que la tabla no cambió, únicamente le agregamos el atributo `grupo`):
```{r}
by_genero <- group_by(df_ej, genero)
by_genero
```
Después operamos sobre cada grupo, creando un resumen a nivel grupo:
```{r}
summarise(by_genero, promedio = mean(estatura))
```


#### Estableciendo el Directorio de Trabajo (_Working Directory_ (WD))
Cuando cargamos archivo, R siempre los llama desde el Directorio de Trabajo. Para consultar cuál es el Directorio de Trabajo actual de nuestra sesión en R utilizamos el comando: 
```{r}
getwd() #Directorio de Trabajo actual
```
Si deseamos cambiar el Directorio de Trabajo, especificamos el nuevo folder mediante el comando:
```{r}
#setwd("C:/Users/lberdicha/Documents/ITAM/Cursos 2017/Otono 2017/Estadistica Computacional/Modulo 2")   #Change WD
getwd() 
```
Para obtener una lista de los archivos que se encuentran en el Directorio de Trabajo:
```{r}
dir() #Lista de archivos en el WD
```


### Limpieza y Reestructuración de Bases de Datos

Los principios de datos limpios parecen obvios pero la mayor parte de las bases de datos no los cumplen debido a que:

1. La mayor parte de la gente no está familiarizada con los principios y es difícil derivarlos por uno mismo.  
2. Los datos suelen estar organizados para facilitar otros aspectos que no son el análisis de datos; por ejemplo, la captura de los datos.  

Algunos de los problemas más comunes en las bases de datos que no están _limpias_ son:

1. Los encabezados de las columnas son valores de otra variable. 
2. Más de una variable por columna. 
3. Las variables están organizadas tanto en filas como en columnas. 
4. Más de un tipo de unidad observacional en una tabla.
5. Una misma unidad observacional está almacenada en múltiples tablas. 

La mayor parte de estos problemas se pueden arreglar con pocas herramientas; 
a continuación veremos como _limpiar_ datos usando 2 funciones del paquete `tidyr`:

* `gather`: recibe múltiples columnas y las junta en pares de valores y nombres, esto es, convierte los datos _anchos_ en _largos_.  
* `spread`: recibe 1 columnas y las separa en 2 o más columnas, haciendo los datos más anchos.

Repasaremos los problemas más comunes que se encuentran en conjuntos de datos
sucios y mostraremos como se puede manipular la tabla de datos (usando las 
funciones *gather* y *spread*) con el fin de estructurarla para que cumpla los
principios de datos limpios.

#### 1. Los encabezados de las columanas son valores de otra variable.
Usaremos ejemplos para entender los conceptos más facilmente.

##### Ejemplo 1: 

La primer base de datos está basada en una encuesta de [Pew Research](http://www.pewforum.org/2009/01/30/income-distribution-within-us-religious-groups/) que investiga la relación entre ingreso y afiliación religiosa.

Cargamos la base de datos de una URL:
```{r}
library(tidyverse)
pew <- read_delim("http://stat405.had.co.nz/data/pew.txt", "\t", 
  escape_double = FALSE, trim_ws = TRUE)
head(pew)
```

¿Cuáles son las variables en estos datos? Esta base de datos tiene 3 variables: religión, ingreso y frecuencia. 

Para _limpiarla_ es necesario apilar las columnas (_alargar_ los datos). Notemos
que al alargar los datos desapareceran las columnas que se agrupan y dan lugar a
dos nuveas columnas: la correspondiente a clave y la correspondiente a valor.
Para alargar una base de datos usamos la función `gather` que recibe 
los argumentos:

* data: base de datos que vamos a reestructurar.  
* key: nombre de la nueva variable que contiene como valores los nombres
de las columnas que vamos a apilar.  
* value: nombre de la variable que almacenará los valores que corresponden a 
cada *key*.  
* ...: lo último que especificamos son las columnas que vamos a apilar, la notación para seleccionarlas es la misma que usamos con `select()`.


```{r}
pew_tidy <- gather(data = pew, income, frequency, -religion) #En este caso indicamos la columna que NO vamos a apilar
head(pew_tidy)
str(pew_tidy)
```

Observemos que en la tabla _ancha_ teníamos bajo la columna *<$10k*, en el renglón
correspondiente a *Agnostic* un valor de 27, y podemos ver que este valor en 
la tabla _larga_ se almacena bajo la columna frecuencia y corresponde a los valores *Agnostic* de religión, y *<$10k* de income. También es importante ver que en este ejemplo especificamos las columnas a apilar identificando la que NO vamos a alargar
con un signo negativo: es decir apila todas las columnas menos religión.

La nueva estructura de la base de datos nos permite, por ejemplo, hacer 
fácilmente una gráfica donde podemos comparar las diferencias en las 
frecuencias. 

Nota: En esta sección no explicaremos las funciones para graficar pues estas 
se cubren en las notas de Visualización en R. En esta parte nos concentramos
en cómo limpiar datos y ejemplificar lo sencillo que es trabajar con datos 
limpios, esto es, una vez que los datos fueron reestructurados e tal forma que cumplan con el principio de datos limpios es fácil construir gráficas y resúmenes estadísticos.


```{r, fig.height = 5.8, fig.width = 6.8, warning = FALSE}
ggplot(pew_tidy, aes(x = income, y = frequency, color = religion, group = religion)) +
  geom_line() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

Podemos hacer gráficas más interesantes si creamos nuevas variables:

```{r, fig.height = 4, fig.width = 7.7}
by_religion <- group_by(pew_tidy, religion)
pew_tidy_2 <- pew_tidy %>%
  filter(income != "Don't know/refused") %>%
  group_by(religion) %>%
  mutate(percent = frequency / sum(frequency)) %>% 
  filter(sum(frequency) > 1000)

head(pew_tidy_2)

ggplot(pew_tidy_2, aes(x = income, y = percent, group = religion)) +
  facet_wrap(~ religion, nrow = 1) +
  geom_bar(stat = "identity", fill = "darkgray") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


##### Ejemplo 2:

Veamos los datos de la tabla `Billboard`, aquí se registra la fecha en la 
que una canción entra por primera vez al top 100 de Billboard.

Cargamos la base de datos de un documento en formato .csv (comma separated values) de nuestro equipo:

```{r}
billboard <- read_csv("data/billboard.csv")
head(billboard)
#str(billboard)
```

Notemos que el rank en cada semana (una vez que entró en la lista) está guardado
en 75 columnas `wk1` a `wk75`, este tipo de almacenamiento no es *limpio* ya que los encabezados corresponden a los valores de la variable semana, pero 
puede ser útil al momento de ingresar la información.

Para tener datos *limpios* apilamos las semanas de manera que sea una sola 
columna (nuevamente _alargamos_ los datos):

```{r}
billboard_long <- gather(billboard, week, rank, wk1:wk76, na.rm = TRUE) #Eliminamos valores faltantes de la variable rank
head(billboard_long)
str(billboard_long)
```

Notemos que en esta ocasión especificamos las columnas que vamos a apilar
indicando el nombre de la primera de ellas seguido de `:` y por último el 
nombre de la última variable a apilar. Por otra parte, la instrucción 
`na.rm = TRUE` se utiliza para eliminar los renglones con valores faltantes en 
la columna de value (rank), esto es, eliminamos aquellas observaciones que 
tenían NA en la columnas wk*num* de la tabla _ancha_. Ahora realizamos una
limpieza adicional creando mejores variables de fecha.


```{r}
billboard_tidy <- billboard_long %>%
  mutate(
    week = parse_number(week),
    date = date.entered + 7 * (week - 1), 
    rank = as.numeric(rank)
    ) %>%
    select(-date.entered)
head(billboard_tidy)
```

Nuevamente, podemos hacer gráficas fácilmente:

```{r, fig.height = 4, fig.width = 7.7}
tracks <- filter(billboard_tidy, track %in% 
    c("Higher", "Amazed", "Kryptonite", "Breathe", "With Arms Wide Open"))

ggplot(tracks, aes(x = date, y = rank)) +
  geom_line() + 
  facet_wrap(~track, nrow = 1) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


#### 2. Una columna asociada a más de una variable
La siguiente base de datos proviene de la Organización Mundial de la Salud (OMS) y 
contiene el número de casos confirmados de tuberculosis por país y año, la
información esta por grupo demográfico de acuerdo a sexo (m, f), y edad (0-4, 
5-14, etc). Los datos están disponibles en http://www.who.int/tb/country/data/download/en/. 


```{r}
tb <- read.csv("data/tb.csv") #Notar que utilizamos la función read.csv en lugar de read_csv
head(tb)
str(tb)
```

**Ejercicio:** De manera similar a los ejemplos anteriores, 
utiliza la función `gather` para apilar las columnas correspondientes a 
género-edad. 

```{r, echo=FALSE, results=FALSE}
tb_long <- gather(tb, demo, n, -iso2, -year, na.rm = TRUE)
head(tb_long)
str(tb_long)
```

Notemos que las variables género y edad se encuentran almacenadas en una sola variable: demo. Para reestructurar la base de datos en una base de datos limpia debemos separar en dos columnas la variable demo, una para almacenar la variable género y otra para la variable edad. Para ello utilizaremos la función `separate()`, esta recibe como parámetros:  

* el nombre de la base de datos,  

* el nombre de la variable que deseamos separar en más de una,  

* la posición de donde deseamos "cortar" (hay más opciones para especificar 
como separar, ver `?separate`). El default es separar valores en todos los lugares que encuentre un caracter que no es alfanumérico (espacio, guión,...).

```{r}
tb_tidy <- separate(tb_long, demo, c("sex", "age"), 8)
head(tb_tidy)
table(tb_tidy$sex) #Frecuencias de las clases de la variable sex
```

Creamos una mejor codificación de la variable género:
```{r}
tb_tidy <- mutate(tb_tidy, sex = substr(sex, 8, 8))
head(tb_tidy)
table(tb_tidy$sex)
```


#### 3. Variables almacenadas en filas y columnas
El problema más difícil es cuando las variables se encuentran almacenadas tanto en filas como encolumnas.

Consideremos una base de datos de clima en Cuernavaca:
```{r}
clima <- read_delim("data/clima.txt", "\t", escape_double = FALSE, trim_ws = TRUE)
head(clima)
```

¿Cuáles son las variables en estos datos? Estos datos tienen variables en columnas individuales (id, año, mes), en múltiples columnas (día, d1-d31) y en filas (tmin, tmax). 

Comencemos por apilar las columnas:
```{r}
clima_long <- gather(clima, day, value, d1:d31, na.rm = TRUE)
head(clima_long)
```

Podemos crear algunas variables adicionales:
```{r}
clima_vars <- clima_long %>% 
  mutate(day = parse_number(day), 
    value = as.numeric(value) / 10) %>%
  select(id, year, month, day, element, value) %>%
  arrange(id, year, month, day)
head(clima_vars)
```

Finalmente, la columna *element* no es una variable, sino que almacena el nombre 
de dos variables, la operación que debemos aplicar es `spread`, la cual es el inverso de 
la función `gather`:

```{r}
clima_tidy <- spread(clima_vars, element, value)
head(clima_tidy)
```

Ahora es inmediato realizar gráficas y ajustar un modelo. Consideremos ajustar una Regresión Linea Simple:
```{r}
# Ajustamos un modelo lineal donde la variable respuesta es temperatura máxima, y la variable explicativa es el mes
clima_lm <- lm(TMAX ~ factor(month), data = clima_tidy)
summary(clima_lm)
```


#### 4. Mas de un tipo de unidad observacional en una misma (Separación o Unión horizontal)
En ocasiones las bases de datos involucran valores en diferentes niveles, en 
diferentes tipos de unidad observacional. En la limpieza de datos, cada unidad
observacional debe estar almacenada en su propia tabla (esto está ligado a 
normalización de una base de datos); es importante para evitar inconsistencias 
en los datos.

Consideremos de nuevo la base de datos `billboard`:

```{r}
head(billboard_tidy)
```

¿Cuáles son las unidades observacionales de los datos de billboard? # Notar que cada canción está asociada a un artista y una duración. Sin embargo, tenemos un ranking de la canción por semana. Para evitar que en la misma tabla se repitan las variables Artista, Año y Duración de manera redundante, separamos la tabla en dos tablas con diferentes unidades observacionales (asumiendo que no existen dos canciones con el mismo nombre):

1. La Variable Canción define una Unidad Observacional. Sus características son Artista, Año y Duración:
```{r}
song <- billboard_tidy %>% 
  select(artist, track, year, time) %>% #Artista, Año y Duración son las características de la canción.
  unique() %>% #Eliminamos observaciones repetidas
  arrange(artist) %>% #Ordenamos por artista
  mutate(song_id = row_number(artist)) #Creamos la variable "song_id" para identificar la Unidad Observacional Canción y ligarla a la segunda tabla
head(song)
```

2. Las Variables Semana y Canción (o "song_id")) definen una Unidad Observacional. Su característica es el Ranking:
```{r}
rank <- billboard_tidy %>%
  left_join(song, c("artist", "track", "year", "time")) %>%
  select(song_id, date, week, rank) %>% #Pegamos la nueva Variable "song_id". Fecha y Semana son las características de la Unidad Observacional
  arrange(song_id, date) %>%
  tbl_df #Si es que es necesario convertir el nuevo Data Frame a formato Tibble
head(rank)
```

En resumen:

* La descomposión de una tabla en términos de subtablas, cada una con un sólo tipo de unidad observacional, es único.
* El almacenamiento de una tabla en términos de las subtablas, cada una con un sólo tipo de unidad observacional, es óptimo.


#### 5. Una misma unidad observacional está almacenada en múltiples tablas (Unión vertical)
También es común que los valores sobre una misma unidad observacional estén 
separados en muchas tablas o archivos; es común que estas tablas esten divididas 
de acuerdo a una variable, de tal manera que cada archivo representa a una 
persona, año o ubicación. Para juntar los archivos hacemos lo siguiente:

1. Leemos los archivos en una lista de tablas. 
2. Para cada tabla agregamos una columna que registra el nombre del archivo original. 
3. Combinamos las tablas en un solo data frame.  

Consideraremos como ejemplo el monitoreo de Contaminación en 332 Ubicaciones de EUA. Cada archivo contiene información de una unidad de monitoreo y el número de identificación del
monitor es el nombre del archivo. Por ejemplo
```{r}
spec_1 <- read_csv("data/specdata/001.csv")
head(spec_1)
```

Los pasos en R (usando el paquete `purrr`):

1. Primero creamos un vector con los nombres de los archivos .csv en un directorio; eligimos aquellos que contengan las
letras ".csv".
```{r, message=FALSE}
paths <- dir("data/specdata", pattern = "\\.csv$", full.names = TRUE) 
head(paths)
```

2. Después le asignamos el nombre del .csv al nombre de cada elemento del vector:
```{r}
paths <- set_names(paths, basename(paths))
head(paths)
```

3. La función `map_df` itera sobre cada dirección, lee el .csv en dicha dirección y los combina en un data frame:
```{r, error=TRUE}
specdata_us <- map_df(paths, ~read_csv(., col_types = "Tddi"), .id = "filename")

# eliminamos la basura del id
specdata <- specdata_us %>%
  mutate(monitor = parse_number(filename)) %>%
  select(id = ID, monitor, date = Date, sulfate, nitrate)
head(specdata)
```

#### Otras consideraciones
En las buenas prácticas es importante tomar en cuenta los siguientes puntos:

* Incluir un encabezado con el nombre de las variables.
* Los nombres de las variables deben ser entendibles (e.g. AgeAtDiagnosis es mejor
que AgeDx).
* En general los datos se deben guardar en un archivo por tabla.
* Escribir un script con las modificaciones que se hicieron a los _datos crudos_ 
(reproducibilidad).

Otros aspectos importantes en la _limpieza_ de datos son: 

* Selección del tipo de variables (por ejemplo fechas).
* Tratamiento de datos faltantes.
* _Typos_ en los la captura de los datos.
* Detección de valores atípicos.



### Divide-aplica-combina (_split-apply-combine_)
Muchos problemas de análisis de datos involucran la aplicación de la estrategia
divide-aplica-combina ([Hadley Whickam, 2011](http://www.jstatsoft.org/v40/i01/paper)). 
Ésta consiste en romper un problema en pedazos (de 
acuerdo a una variable de interés), operar sobre cada subconjunto de manera
independiente (ej. calcular la media de cada grupo, ordenar observaciones por 
grupo, estandarizar por grupo) y después unir los pedazos nuevamente. La estrategia consiste en:

* **Divide** la base de datos original.  
* **Aplica** funciones a cada subconjunto.  
* **Combina** los resultados en una nueva base de datos.


El siguiente diagrama ejemplifica el paradigma de divide-aplica-combina:

![](imagenes/split-apply-combine.png) 

En esta sección trabajaremos con las siguientes bases de datos para ejemplifcar las funciones de divide-aplica-combina:

```{r, warning=FALSE}
flights <- read_csv("data/flights.csv")
flights

weather <- read_csv("data/weather.csv")
weather 

planes <- read_csv("data/planes.csv")
planes

airports <- read_csv("data/airports.csv")
airports
```

Cuando pensamos en cómo implementar la estrategia divide-aplica-combina es natural pensar en iteraciones; por ejemplo, utilizar un ciclo _for_ para recorrer cada 
grupo de interés y aplicar las funciones; sin embargo, la aplicación de ciclos 
_for_ desemboca en código difícil de entender. Adicionalmente, el paquete `dplyr` es mucho 
más veloz mediante el uso de las funciones `filter`, `select`, `arrange`, `mutate` y `summarise`.

Como ya vimos al inicio de estas notas, estas funciones trabajan de manera similar: el primer argumento que reciben es un _data frame_ (usualmente en formato *limpio*), los argumentos que siguen
indican que operación se va a efectuar y el resultado es un nuevo _data frame_.

Adicionalmente, se pueden usar con la función `group_by` que cambia el dominio de cada 
función, pasando de operar en el conjunto de datos completos a operar en subgrupos.

Para profundizar en el uso de estas funciones consideraremos más ejemplos (ejercicios):

#### Filtrar
Elejir filas de un conjunto de datos.

Algunos operadores importantes para filtrar son:  

```{r, eval = FALSE}
x > 1
x >= 1
x < 1
x <= 1
x != 1
x == 1
x %in% ("a", "b")
```

Debemos tener cuidado al usar `==`

```{r}
sqrt(2) ^ 2 == 2
1/49 * 49 == 1
```
Los resultados de arriba se deben a que las computadoras 
usan aritmética de precisión finita:

```{r}
print(1/49 * 49, digits = 20)
```

Para estos casos es útil usar la función `near()`

```{r}
near(sqrt(2) ^ 2,  2)
near(1 / 49 * 49, 1)
```

Los operadores booleanos también son convenientes para 
filtrar:

```{r, eval = FALSE}
# Conjuntos
a | b
a & b
a & !b
xor(a, b)
```

El siguiente esquema nos ayuda a entender que hace cada operación:
```{r, out.width = "400px"}
knitr::include_graphics("imagenes/transform-logical.png")
```

**Ejercicio:**

1. Encuentra todos los vuelos hacia SFO ó OAK.
2. Encuentra los vuelos con un retraso mayor a una hora.
3. Encuentra los vuelos en los que el retraso de llegada es más del doble que el retraso de salida.


#### Seleccionar
Elegir columnas de un conjunto de datos.

**Ejercicio:** Ve la ayuda de select (`?select`) y escribe tres maneras de seleccionar las variables de retraso (delay).

#### Arreglar
Arreglar u ordenar de acuerdo al valor de una o más variables:

**Ejercicio:**

1. Ordena los vuelos por fecha de salida y hora.
2. ¿Cuáles son los vuelos con mayor retraso?
3. ¿Qué vuelos "ganaron"" más tiempo en el aire?

#### Mutar
Mutar consiste en crear nuevas variables aplicando una función a columnas existentes:

**Ejercicio:** 

1. Calcula la velocidad en millas por hora a partir de
la variable tiempo y la distancia (en millas). ¿Quá vuelo fue el más rápido?
2. Crea una nueva variable que muestre cuánto tiempo se ganó o perdió durante el vuelo.

Hay muchas funciones que podemos usar para crear nuevas variables con `mutate()`, éstas deben cumplir con la propiedad de ser funciones vectorizadas, es decir, reciben un vector de valores y devuelven un vector de la misma dimensión.

#### Summarise y resúmenes por grupo
Summarise sirve para crear nuevas bases de datos con resúmenes o agregaciones de 
los datos originales.

**Ejercicio:**

1.Calcula el retraso promedio por fecha.
2. ¿Qué otros resúmenes puedes hacer para explorar el retraso por fecha?

Algunas funciones útiles con _summarise_ son `min()`, `max()`, `n()`, `sum()`, `mean()`, `sd()`, `median()`, `quantile()`.

Por ejemplo:
```{r}
flights$date_only <- as.Date(flights$date)
by_date <- group_by(flights, date_only)
no_miss <- filter(by_date, !is.na(dep))
delays <- summarise(no_miss, mean_delay = mean(dep_delay), n = n())
```


#### Operador pipeline
En R, cuando uno hace varias operaciones, es difícil leer y entender el código: 

```{r}
hourly_delay <- filter(summarise(group_by(filter(flights, !is.na(dep_delay)), 
  date_only, hour), delay = mean(dep_delay), n = n()), n > 10)
```

La dificultad radica en que usualmente los parámetros se asignan después del 
nombre de la función usando () y las operaciones definidas por las funciones concatenadas se ejecutan "de adentro hacia afuera" en los paréntesis.

El operador "Forward Pipe" (`%>%`) cambia este orden, de manera que un parámetro que precede a la función es enviado (*piped*) a la función. Por ejemplo:

* `x %>% f(y)` se vuelve `f(x,y)`,  
* `x %>% f(y) %>% g(z)` se vuelve `g(f(x, y), z)`. 

De esta forma podemos reescribir el código para poder leer las operaciones que vamos aplicando de izquierda a derecha y de arriba hacia abajo.

Veamos como cambia el código anterior:

```{r}
hourly_delay <- flights %>%
  filter(!is.na(dep_delay)) %>%
  group_by(date_only, hour) %>%
  summarise(delay = mean(dep_delay), n = n()) %>%
  filter(n > 10)
```
Intuitivamente podemos leer `%>%` como "después".

**Ejercicio:**

1. ¿Qué destinos tienen el promedio de retrasos más alto?
2. ¿Qué vuelos (compañía + vuelo) ocurren diariamente?
3. En promedio, ¿cómo varían a lo largo del día los retrasos de vuelos no cancelados? (*Sugerencia*: hour +
minute / 60)

#### Variables por grupo
En ocasiones es conveniente crear variables por grupo, por ejemplo estandarizar
dentro de cada grupo `z = (x - mean(x)) / sd(x)`.

Veamos un ejemplo:
```{r}
planes <- flights %>%
  filter(!is.na(arr_delay)) %>%
  group_by(plane) %>%
  filter(n() > 30)

planes %>%
  mutate(z_delay =
    (arr_delay - mean(arr_delay)) / sd(arr_delay)) %>%
  filter(z_delay > 5)
```

#### Verbos de dos tablas
¿Cómo mostramos los retrasos de los vuelos en un mapa? Para responder esta pregunta necesitamos unir la base de datos de vuelos con la de aeropuertos:

```{r}
location <- airports %>%
  select(dest = iata, name = airport, lat, long)

flights %>%
  group_by(dest) %>%
  filter(!is.na(arr_delay)) %>%
  summarise(
    arr_delay = mean(arr_delay),
    n = n() ) %>%
    arrange(desc(arr_delay)) %>%
    left_join(location)
```

Hay varias maneras de unir dos bases de datos y debemos pensar en el 
obejtivo:

```{r}
x <- tibble(name = c("John", "Paul", "George", "Ringo", "Stuart", "Pete"),
  instrument = c("guitar", "bass", "guitar", "drums", "bass",
     "drums"))

y <- tibble(name = c("John", "Paul", "George", "Ringo", "Brian"),
  band = c("TRUE", "TRUE", "TRUE",  "TRUE", "FALSE"))
x
y

inner_join(x, y)
left_join(x, y)
semi_join(x, y)
anti_join(x, y)
```

Resumamos lo que observamos arriba:

Tipo | Acción
-----|-------
inner|Incluye únicamente las filas que aparecen tanto en x como en y
left |Incluye todas las filas en x y las filas de y que coincidan
semi |Incluye las filas de x que coincidan con y
anti |Incluye las filas de x que no coinciden con y

Ahora combinamos datos a nivel hora con condiciones climáticas. ¿Cuál es el tipo de unión adecuado?

```{r}
hourly_delay <- flights %>%
  group_by(date_only, hour) %>%
  filter(!is.na(dep_delay)) %>%
  summarise(
    delay = mean(dep_delay),
    n = n() ) %>%
  filter(n > 10)

delay_weather <- hourly_delay %>% left_join(weather)

arrange(delay_weather, -delay)
```

**Ejercicio:** 

1. ¿Qué condiciones climáticas están asociadas con retrasos en las salidas de Houston?
2. Explora si los aviones más viejos están asociados a mayores retrasos, responde con una gráfica.

****

### Recursos adicionales

* [Tidy Data](http://vita.had.co.nz/papers/tidy-data.pdf), Hadley Wickham.

* [The Slit-Apply-Combine Strategy for Data Analysis](http://www.jstatsoft.org/v40/i01/paper), 
Hadley Wickham.

* [Data Wrangling Cheat Sheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf), 
RStudio.